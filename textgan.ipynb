{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, LSTM\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaDiscriminator:\n",
    "    def __init__(self, max_sequence_len):\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "    \n",
    "    def build_model(self):\n",
    "        txt_shape = (self.max_sequence_len, 1, 1)\n",
    "        model = Sequential(name='VanillaDiscriminator')\n",
    "        model.add(Flatten(input_shape=txt_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        txt = Input(shape=txt_shape)\n",
    "        validity = model(txt)\n",
    "        self.discriminator = Model(txt, validity)\n",
    "        return self.discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGenerator:\n",
    "    def __init__(self, max_sequence_len):\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "    \n",
    "    def build_model(self):\n",
    "        noise_shape = (self.max_sequence_len, 1)\n",
    "        model = Sequential(name='VanillaGenerator')\n",
    "        model.add(LSTM(256, input_shape=noise_shape))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(noise_shape[0], activation='tanh'))\n",
    "        model.summary()\n",
    "        noise = Input(shape=noise_shape)\n",
    "        txt = model(noise)\n",
    "        self.generator = Model(noise, txt)\n",
    "        return self.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGAN:\n",
    "    def __init__(self, discriminator, generator, max_sequence_len):\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.discriminator.trainable = False # it's important to freeze the discriminator when training the generator\n",
    "        gan_input = Input(shape=(self.max_sequence_len, 1)) # The GAN takes noise as input \n",
    "        generator_out = self.generator(gan_input) # generates text output\n",
    "        gan_output = self.discriminator(generator_out) # and validates generated text \n",
    "        self.gan =  Model(gan_input, gan_output, name='GAN')\n",
    "        self.gan.summary()\n",
    "        return self.gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANTrainer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, X_train, discriminator, generator, gan, batch_size, epochs):\n",
    "        half_batch = int(batch_size/2)\n",
    "        for epoch in range(epochs):\n",
    "            ##########################\n",
    "            # train the discriminator on half-real and half-fake data\n",
    "            ##########################\n",
    "            # get random half-batch real data\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            txts = X_train[idx]\n",
    "\n",
    "            # get half-batch fake data\n",
    "            noise = np.random.normal(0, 1, (half_batch, columns, 1))\n",
    "            gen_txts = generator.predict(noise)\n",
    "            gen_txts = np.expand_dims(gen_txts, axis=2)\n",
    "            gen_txts = np.expand_dims(gen_txts, axis=3)\n",
    "\n",
    "            # compute discriminator losses on real and fake data and average them\n",
    "            d_loss_real = discriminator.train_on_batch(txts, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_txts, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            \n",
    "            ##########################\n",
    "            # train the GAN, thereby traning the generator (on full-batch data)\n",
    "            # the discriminator is not trained in the GAN because it's trainable flag is set to False \n",
    "            ##########################\n",
    "            noise = np.random.normal(0, 1, (batch_size, columns, 1))\n",
    "            # the generator wants discriminator to mistake texts as real. Therefore send np.ones as labels\n",
    "            g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 768 1\n",
      "(7613, 768, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# data loading and preprocessing\n",
    "data = pickle.load(open(\"X.p\",\"rb\"))\n",
    "X_train = data\n",
    "rows, columns, channels = X_train.shape\n",
    "print(rows, columns, channels)\n",
    "# expand the last dimension\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter initializations\n",
    "max_sequence_len = columns\n",
    "batch_size = 1024\n",
    "epochs = 100\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VanillaDiscriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = VanillaDiscriminator(max_sequence_len).build_model()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VanillaGenerator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 768)               197376    \n",
      "=================================================================\n",
      "Total params: 461,568\n",
      "Trainable params: 461,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = VanillaGenerator(max_sequence_len).build_model()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GAN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 768, 1)]          0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 768)               461568    \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 1)                 525313    \n",
      "=================================================================\n",
      "Total params: 986,881\n",
      "Trainable params: 461,568\n",
      "Non-trainable params: 525,313\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan = TextGAN(discriminator, generator, max_sequence_len).build_model()\n",
    "gan.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.704702, acc.: 35.55%] [G loss: 0.691149]\n",
      "1 [D loss: 0.491764, acc.: 50.00%] [G loss: 0.688128]\n",
      "2 [D loss: 0.416639, acc.: 50.00%] [G loss: 0.683806]\n",
      "3 [D loss: 0.390614, acc.: 50.00%] [G loss: 0.679571]\n",
      "4 [D loss: 0.380743, acc.: 50.00%] [G loss: 0.675496]\n",
      "5 [D loss: 0.377441, acc.: 50.00%] [G loss: 0.670860]\n",
      "6 [D loss: 0.377320, acc.: 50.00%] [G loss: 0.666329]\n",
      "7 [D loss: 0.378216, acc.: 50.00%] [G loss: 0.661325]\n",
      "8 [D loss: 0.380268, acc.: 50.00%] [G loss: 0.655204]\n",
      "9 [D loss: 0.384612, acc.: 50.00%] [G loss: 0.647454]\n",
      "10 [D loss: 0.391916, acc.: 50.00%] [G loss: 0.634348]\n",
      "11 [D loss: 0.417960, acc.: 50.00%] [G loss: 0.610074]\n",
      "12 [D loss: 0.672615, acc.: 50.00%] [G loss: 0.524856]\n",
      "13 [D loss: 0.706141, acc.: 50.00%] [G loss: 1.293922]\n",
      "14 [D loss: 0.210587, acc.: 100.00%] [G loss: 2.240002]\n",
      "15 [D loss: 0.180414, acc.: 100.00%] [G loss: 1.991547]\n",
      "16 [D loss: 0.279523, acc.: 99.51%] [G loss: 1.067211]\n",
      "17 [D loss: 0.330659, acc.: 93.26%] [G loss: 0.802134]\n",
      "18 [D loss: 0.336095, acc.: 91.11%] [G loss: 0.768095]\n",
      "19 [D loss: 0.332248, acc.: 92.87%] [G loss: 0.761780]\n",
      "20 [D loss: 0.331307, acc.: 93.55%] [G loss: 0.762597]\n",
      "21 [D loss: 0.328462, acc.: 97.66%] [G loss: 0.764663]\n",
      "22 [D loss: 0.324446, acc.: 99.61%] [G loss: 0.767718]\n",
      "23 [D loss: 0.322611, acc.: 99.71%] [G loss: 0.774004]\n",
      "24 [D loss: 0.319391, acc.: 100.00%] [G loss: 0.781106]\n",
      "25 [D loss: 0.315512, acc.: 100.00%] [G loss: 0.786994]\n",
      "26 [D loss: 0.313194, acc.: 100.00%] [G loss: 0.790979]\n",
      "27 [D loss: 0.309928, acc.: 100.00%] [G loss: 0.799332]\n",
      "28 [D loss: 0.306891, acc.: 100.00%] [G loss: 0.806745]\n",
      "29 [D loss: 0.303697, acc.: 100.00%] [G loss: 0.810987]\n",
      "30 [D loss: 0.301474, acc.: 100.00%] [G loss: 0.817992]\n",
      "31 [D loss: 0.300091, acc.: 100.00%] [G loss: 0.824373]\n",
      "32 [D loss: 0.297146, acc.: 100.00%] [G loss: 0.830094]\n",
      "33 [D loss: 0.294448, acc.: 100.00%] [G loss: 0.834783]\n",
      "34 [D loss: 0.291491, acc.: 100.00%] [G loss: 0.840835]\n",
      "35 [D loss: 0.288227, acc.: 100.00%] [G loss: 0.849398]\n",
      "36 [D loss: 0.285072, acc.: 100.00%] [G loss: 0.861097]\n",
      "37 [D loss: 0.281976, acc.: 100.00%] [G loss: 0.870302]\n",
      "38 [D loss: 0.277976, acc.: 100.00%] [G loss: 0.879607]\n",
      "39 [D loss: 0.274134, acc.: 100.00%] [G loss: 0.892205]\n",
      "40 [D loss: 0.270240, acc.: 100.00%] [G loss: 0.909006]\n",
      "41 [D loss: 0.264233, acc.: 100.00%] [G loss: 0.922804]\n",
      "42 [D loss: 0.258409, acc.: 100.00%] [G loss: 0.939694]\n",
      "43 [D loss: 0.252716, acc.: 100.00%] [G loss: 0.959231]\n",
      "44 [D loss: 0.246929, acc.: 100.00%] [G loss: 0.981673]\n",
      "45 [D loss: 0.240109, acc.: 100.00%] [G loss: 1.003116]\n",
      "46 [D loss: 0.233522, acc.: 100.00%] [G loss: 1.025825]\n",
      "47 [D loss: 0.227263, acc.: 100.00%] [G loss: 1.053005]\n",
      "48 [D loss: 0.221013, acc.: 100.00%] [G loss: 1.072423]\n",
      "49 [D loss: 0.216111, acc.: 100.00%] [G loss: 1.094272]\n",
      "50 [D loss: 0.209816, acc.: 100.00%] [G loss: 1.116282]\n",
      "51 [D loss: 0.204596, acc.: 100.00%] [G loss: 1.134056]\n",
      "52 [D loss: 0.200534, acc.: 100.00%] [G loss: 1.154118]\n",
      "53 [D loss: 0.196051, acc.: 100.00%] [G loss: 1.172778]\n",
      "54 [D loss: 0.192443, acc.: 100.00%] [G loss: 1.190872]\n",
      "55 [D loss: 0.188024, acc.: 100.00%] [G loss: 1.213576]\n",
      "56 [D loss: 0.183594, acc.: 100.00%] [G loss: 1.230015]\n",
      "57 [D loss: 0.178754, acc.: 100.00%] [G loss: 1.253800]\n",
      "58 [D loss: 0.174235, acc.: 100.00%] [G loss: 1.277109]\n",
      "59 [D loss: 0.170462, acc.: 100.00%] [G loss: 1.302082]\n",
      "60 [D loss: 0.165187, acc.: 100.00%] [G loss: 1.329496]\n",
      "61 [D loss: 0.160813, acc.: 100.00%] [G loss: 1.357046]\n",
      "62 [D loss: 0.155941, acc.: 100.00%] [G loss: 1.387180]\n",
      "63 [D loss: 0.150407, acc.: 100.00%] [G loss: 1.419305]\n",
      "64 [D loss: 0.145684, acc.: 100.00%] [G loss: 1.449297]\n",
      "65 [D loss: 0.140291, acc.: 100.00%] [G loss: 1.482630]\n",
      "66 [D loss: 0.136566, acc.: 100.00%] [G loss: 1.511926]\n",
      "67 [D loss: 0.132542, acc.: 100.00%] [G loss: 1.541234]\n",
      "68 [D loss: 0.129031, acc.: 100.00%] [G loss: 1.568000]\n",
      "69 [D loss: 0.125091, acc.: 100.00%] [G loss: 1.595445]\n",
      "70 [D loss: 0.122461, acc.: 100.00%] [G loss: 1.620008]\n",
      "71 [D loss: 0.119209, acc.: 100.00%] [G loss: 1.646542]\n",
      "72 [D loss: 0.116076, acc.: 100.00%] [G loss: 1.672502]\n",
      "73 [D loss: 0.113840, acc.: 100.00%] [G loss: 1.702274]\n",
      "74 [D loss: 0.110948, acc.: 100.00%] [G loss: 1.728948]\n",
      "75 [D loss: 0.108490, acc.: 100.00%] [G loss: 1.758911]\n",
      "76 [D loss: 0.106166, acc.: 100.00%] [G loss: 1.788666]\n",
      "77 [D loss: 0.104239, acc.: 100.00%] [G loss: 1.819144]\n",
      "78 [D loss: 0.102981, acc.: 100.00%] [G loss: 1.850709]\n",
      "79 [D loss: 0.100994, acc.: 100.00%] [G loss: 1.881684]\n",
      "80 [D loss: 0.099397, acc.: 100.00%] [G loss: 1.915515]\n",
      "81 [D loss: 0.096235, acc.: 100.00%] [G loss: 1.947258]\n",
      "82 [D loss: 0.094028, acc.: 100.00%] [G loss: 1.978711]\n",
      "83 [D loss: 0.092832, acc.: 100.00%] [G loss: 2.005202]\n",
      "84 [D loss: 0.091543, acc.: 100.00%] [G loss: 2.039434]\n",
      "85 [D loss: 0.089900, acc.: 100.00%] [G loss: 2.088504]\n",
      "86 [D loss: 0.086432, acc.: 100.00%] [G loss: 2.166107]\n",
      "87 [D loss: 0.082773, acc.: 99.80%] [G loss: 2.228372]\n",
      "88 [D loss: 0.072259, acc.: 100.00%] [G loss: 2.284426]\n",
      "89 [D loss: 0.067815, acc.: 100.00%] [G loss: 2.321332]\n",
      "90 [D loss: 0.063723, acc.: 100.00%] [G loss: 2.350923]\n",
      "91 [D loss: 0.062289, acc.: 100.00%] [G loss: 2.374824]\n",
      "92 [D loss: 0.061376, acc.: 100.00%] [G loss: 2.393699]\n",
      "93 [D loss: 0.061598, acc.: 99.90%] [G loss: 2.415648]\n",
      "94 [D loss: 0.061051, acc.: 100.00%] [G loss: 2.453385]\n",
      "95 [D loss: 0.059556, acc.: 100.00%] [G loss: 2.510245]\n",
      "96 [D loss: 0.058082, acc.: 99.90%] [G loss: 2.558117]\n",
      "97 [D loss: 0.060869, acc.: 100.00%] [G loss: 2.434706]\n",
      "98 [D loss: 0.008048, acc.: 99.90%] [G loss: 6.139843]\n",
      "99 [D loss: 0.088321, acc.: 100.00%] [G loss: 7.820413]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "GANTrainer().train(X_train, discriminator, generator, gan, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fibber]",
   "language": "python",
   "name": "conda-env-.conda-fibber-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
